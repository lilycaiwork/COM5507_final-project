{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Data Harvesting\n",
    "## White House Website News\n",
    "### https://www.whitehouse.gov/news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import bs4 as bs \n",
    "\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harvest White House Website's News with their Dates, Titles, Urls.\n",
    "titles = []\n",
    "dates = []\n",
    "pages = [str(i) for i in range(1,487)] # When we were doing this project, there were 487 pages of news.\n",
    "urls =[]\n",
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "for page in pages:\n",
    "    source = requests.get('https://www.whitehouse.gov/news/page/'+ page, headers = headers)\n",
    "    sleep(randint(1,2))\n",
    "    page_html = bs.BeautifulSoup(source.content, 'html.parser')\n",
    "    news_containers = page_html.find_all('article')  \n",
    "    for container in news_containers:\n",
    "        for title in container.find_all('h2'): #titles\n",
    "            titles.append(title.text.strip()) \n",
    "        for date in container.find_all('p', class_=\"meta__date\"): #dates\n",
    "            dates.append(date.text.strip())\n",
    "        for url in container.find_all('h2'): #urls\n",
    "            for url2 in url.find_all('a'):\n",
    "                link = url2.get(\"href\")\n",
    "                urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harvest all full-texts from Urls acquired in the first step.\n",
    "texts = []\n",
    "for url in urls:\n",
    "    source = requests.get(url)\n",
    "    sleep(randint(1,2))\n",
    "    page_html = bs.BeautifulSoup(source.content, 'html.parser')\n",
    "    news_containers = page_html.find('div', class_=\"page-content__content editor\")\n",
    "    clean = news_containers.text.replace('Share:','') # remove the unwanted texts\n",
    "    clean = clean.replace('share-this-page-on-facebook', '') # remove the unwanted texts\n",
    "    clean = clean.replace('share-this-page-on-twitter', '') # remove the unwanted texts\n",
    "    clean = clean.replace('copy-url-to-your-clipboard','') # remove the unwanted texts\n",
    "    clean = clean.replace('All News', '').strip() # remove the unwanted texts\n",
    "    clean = clean.replace(\"\\n\",'') # remove the unwanted texts\n",
    "    if '\\xa0' in clean: # remove the unwanted texts\n",
    "        clean = clean.replace('\\xa0','')\n",
    "        texts.append(clean)\n",
    "    else:texts.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary from which a dataframe is created subsequently.\n",
    "pd_us_gov_total = {\n",
    "    'dates' : dates,\n",
    "    'titles' : titles,\n",
    "    'urls' : urls,\n",
    "    'texts' : texts\n",
    "}\n",
    "pd_us_gov_news_total = pd.DataFrame.from_dict(pd_us_gov_total, orient = 'index').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Council of China Website News\n",
    "### http://english.gov.cn/news/topnews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import bs4 as bs \n",
    "\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harvest White House Website's News with their Dates, Titles, Urls.\n",
    "titles = []\n",
    "dates = []\n",
    "urls = []\n",
    "texts = []\n",
    "\n",
    "\n",
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "\n",
    "source = requests.get(\"http://english.gov.cn/news/topnews/\", headers = headers) \n",
    "\n",
    "page_html = bs.BeautifulSoup(source.content, 'html.parser')\n",
    "\n",
    "#display=block\n",
    "news_containers = page_html.find_all('div', style = 'display:block') \n",
    "for container in news_containers:\n",
    "\n",
    "    for title in container.find_all('a'): \n",
    "        titles.append(title.text.strip())\n",
    "    for date in container.find_all('span'):\n",
    "        dates.append(date.text.strip())\n",
    "    for url in container.find_all('a'):\n",
    "        link = \"http://english.gov.cn\" + url.get('href')\n",
    "        urls.append(link)\n",
    "\n",
    "#display=none\n",
    "news_containers2 = page_html.find_all('div', style = 'display:none')        \n",
    "for container in news_containers2:\n",
    "\n",
    "    for title in container.find_all('a'): \n",
    "        titles.append(title.text.strip())\n",
    "    for date in container.find_all('span'):\n",
    "        dates.append(date.text.strip())\n",
    "    for url in container.find_all('a'):\n",
    "        link = \"http://english.gov.cn\" + url.get('href')\n",
    "        urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harvest all full-texts from Urls acquired in the first step.\n",
    "for url in new_urls:\n",
    "    try: # There are some invalid urls, so try/except statements are used to skip them.\n",
    "        source = requests.get(url)\n",
    "        page_html = bs.BeautifulSoup(source.content, 'html.parser')\n",
    "        news_containers = page_html.find('content')\n",
    "        new_texts.append(news_containers.text)\n",
    "    except:\n",
    "        text = 'no text'\n",
    "        new_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary from which a dataframe is created subsequently.\n",
    "cn_gov_news = {'dates':dates,\n",
    "                  'titles':titles,\n",
    "                  'urls':urls,\n",
    "                  'texts':texts\n",
    "}\n",
    "pd_cn_gov_news = pd.DataFrame.from_dict(cn_gov_news, orient = 'index').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe into a CSV file.\n",
    "pd_cn_gov_news.to_csv('cn_gwy_total.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part two: Data Cleaning\n",
    "### White House\n",
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('white_house_news.csv')\n",
    "df1_titles = pd.DataFrame(df1['titles'], columns=['titles'])\n",
    "In [3]:\n",
    "df1_titles['titles'] = df1_titles['titles'].fillna('')\n",
    "df1_titles['titles'] = df1_titles['titles'].str.replace('\\n','')\n",
    "df1_titles['titles'] = df1_titles['titles'].str.replace('\\t','')\n",
    "df1_titles['titles'] = df1_titles['titles'].str.replace('-',' ')\n",
    "df1_titles['titles'] = df1_titles['titles'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df1_titles['titles'] = df1_titles['titles'].str.replace('[^\\w\\s]','')\n",
    "In [4]:\n",
    "import numpy as np\n",
    "df1_titles['titles'] = df1_titles['titles'].str.replace('\\d+', '')\n",
    "df1_titles['titles'].replace(' ', np.nan, inplace=True)\n",
    "In [5]:\n",
    "from textblob import Word\n",
    "df1_titles['titles'] = df1_titles['titles'].apply(lambda x: \" \".join([Word(word).lemmatize('v') for word in x.split()]))\n",
    "df1_titles['titles'] = df1_titles['titles'].apply(lambda x: \" \".join([Word(word).lemmatize('n') for word in x.split()]))\n",
    "In [6]:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df1_titles['titles'] = df1_titles['titles'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "In [7]:\n",
    "df1_titles['titles'] = df1_titles['titles'].str.findall('\\w{2,}').str.join(' ')\n",
    "In [8]:\n",
    "df1_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_texts = pd.DataFrame(df1['texts'], columns=['texts'])\n",
    "In [10]:\n",
    "df1_texts['texts'] = df1_texts['texts'].fillna(\"\")\n",
    "df1_texts['texts'] = df1_texts['texts'].str.replace('\\n',' ')\n",
    "df1_texts['texts'] = df1_texts['texts'].str.replace('\\t',' ')\n",
    "df1_texts['texts'] = df1_texts['texts'].str.replace('[^\\w\\s]',' ')\n",
    "df1_texts['texts'] = df1_texts['texts'].str.replace('[ ]{2,}',' ')\n",
    "In [11]:\n",
    "df1_texts['texts'] = df1_texts['texts'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "In [12]:\n",
    "import numpy as np\n",
    "df1_texts['texts'] = df1_texts['texts'].str.replace('\\d+', '')\n",
    "df1_texts['texts'].replace(' ', np.nan, inplace=True)\n",
    "In [13]:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df1_texts['texts'] = df1_texts['texts'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "In [14]:\n",
    "from textblob import Word\n",
    "df1_texts['texts'] = df1_texts['texts'].apply(lambda x: \" \".join([Word(word).lemmatize('v') for word in x.split()]))\n",
    "df1_texts['texts'] = df1_texts['texts'].apply(lambda x: \" \".join([Word(word).lemmatize('n') for word in x.split()]))\n",
    "In [15]:\n",
    "df1_texts['texts'] = df1_texts['texts'].str.findall('\\w{2,}').str.join(' ')\n",
    "In [16]:\n",
    "df1_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create a cleaned dataframe\n",
    "df1_dates = df1.dates\n",
    "In [18]:\n",
    "date_series1 = pd.to_datetime(df1_dates)\n",
    "In [19]:\n",
    "date_series1 = date_series1.dt.date\n",
    "In [20]:\n",
    "date_series1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = date_series2.tolist()\n",
    "f = df2_titles['titles'].tolist()\n",
    "g = df2_texts['texts'].tolist()\n",
    "pd_cn_news_total = {'dates':e,'titles':f,'texts':g}\n",
    "In [46]:\n",
    "CHN = pd.DataFrame.from_dict(pd_cn_news_total, orient='index').transpose()\n",
    "In [47]:\n",
    "CHN.to_csv('CHN_Cleaned.csv', encoding='utf-8')\n",
    "In [48]:\n",
    "CHN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "In [2]:\n",
    "datafile1 = 'USA_Cleaned.csv'\n",
    "raw_data1 = pd.read_csv(datafile1, parse_dates=[1], infer_datetime_format=True)\n",
    "In [3]:\n",
    "to_drop1 = ['Unnamed: 0','texts']\n",
    "raw_data1.drop(to_drop1, inplace=True, axis=1)\n",
    "In [4]:\n",
    "reindexed_data1 = raw_data1['titles'].fillna('')\n",
    "reindexed_data1.index = raw_data1['dates']\n",
    "In [5]:\n",
    "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
    "    vectorized_headlines = count_vectorizer.fit_transform(text_data.as_matrix())\n",
    "    \n",
    "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
    "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
    "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
    "    \n",
    "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
    "    for i in range(n_top_words):\n",
    "        word_vectors[i,word_indices[0,i]] = 1\n",
    "\n",
    "    words = [word[0].encode('ascii').decode('utf-8') for word in count_vectorizer.inverse_transform(word_vectors)]\n",
    "\n",
    "    return (words, word_values[0,:n_top_words].tolist()[0])\n",
    "In [6]:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "words, word_values = get_top_n_words(n_top_words=20, count_vectorizer=count_vectorizer, text_data=reindexed_data1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24,8))\n",
    "ax.bar(range(len(words)), word_values)\n",
    "ax.set_xticks(range(len(words)))\n",
    "ax.set_xticklabels(words)\n",
    "ax.set_title('Top 20 Words in Titles: White House')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_counts = reindexed_data1.resample('M').count()\n",
    "daily_counts = reindexed_data1.resample('D').count()\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(24,16))\n",
    "ax[0].plot(daily_counts)\n",
    "ax[0].set_title('White House News Daily Counts')\n",
    "ax[1].plot(monthly_counts)\n",
    "ax[1].set_title('White House News Monthly Counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "In [12]:\n",
    "df_usa = pd.read_csv('USA_Cleaned.csv')\n",
    "df_usa_titles = pd.DataFrame(df_usa['titles'], columns=['titles'])\n",
    "df_usa_titles['titles'] = df_usa['titles'].fillna('')\n",
    "In [13]:\n",
    "text_usa = \" \".join(title for title in df_usa_titles['titles'])\n",
    "In [14]:\n",
    "wordcloud = WordCloud(width=1600,height=800,max_words=100,background_color=\"white\",collocations=False).generate(text_usa)\n",
    "plt.figure( figsize=(20,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LDA\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa = pd.read_csv('USA_Cleaned.csv',encoding='utf-8')\n",
    "In [17]:\n",
    "df_usa_texts = df_usa.texts.fillna('')\n",
    "In [18]:\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "In [19]:\n",
    "tokenized_usa_texts = df_usa_texts.apply(lambda x: x.split())\n",
    "In [20]:\n",
    "dictionary_usa = gensim.corpora.Dictionary(tokenized_usa_texts)\n",
    "In [21]:\n",
    "bow_corpus_usa = [dictionary_usa.doc2bow(doc) for doc in tokenized_usa_texts]\n",
    "In [22]:\n",
    "from gensim import corpora, models\n",
    "tfidf_usa = models.TfidfModel(bow_corpus_usa)\n",
    "corpus_tfidf_usa = tfidf_usa[bow_corpus_usa]\n",
    "In [23]:\n",
    "#LDA using bag of words\n",
    "lda_model_usa = gensim.models.LdaMulticore(bow_corpus_usa, num_topics=6, id2word=dictionary_usa, passes=5, workers=2)\n",
    "In [24]:\n",
    "for idx, topic in lda_model_usa.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA using tf-idf\n",
    "lda_model_tfidf_usa = gensim.models.LdaMulticore(corpus_tfidf_usa, num_topics=6, id2word=dictionary_usa, passes=5, workers=2)\n",
    "In [26]:\n",
    "for idx, topic in lda_model_tfidf_usa.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
